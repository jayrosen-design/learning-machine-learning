export interface Flashcard {
  id: number;
  question: string;
  answer: string;
  category?: string;
}

export const flashcardsData: Flashcard[] = [
  { id: 1, question: "What specialized field of AI focuses on creating entirely new content rather than just analyzing existing data?", answer: "Generative AI (GenAI)", category: "Foundations" },
  { id: 2, question: "What is a large-scale neural network trained on vast amounts of unlabeled data that can be adapted to many downstream tasks?", answer: "Foundation Model", category: "Foundations" },
  { id: 3, question: "A Large Language Model (LLM) is typically trained on datasets measured in what scale of size?", answer: "Terabytes", category: "Foundations" },
  { id: 4, question: "What type of AI model can simultaneously process and generate multiple data types like text, images, and video?", answer: "Multimodal Model", category: "Foundations" },
  { id: 5, question: "Which deep learning architecture uses self-attention mechanisms to handle long-range dependencies in sequential data?", answer: "Transformer", category: "Architecture" },
  { id: 6, question: "What component of the Transformer architecture allows the model to weigh the relative importance of different input parts?", answer: "Attention Mechanism", category: "Architecture" },
  { id: 7, question: "What is the fundamental building block of a neural network that acts as a simple binary classifier?", answer: "Perceptron", category: "Neural Networks" },
  { id: 8, question: "A feedforward neural network with fully connected neurons and hidden layers is known as a _____.", answer: "Multilayer Perceptron (MLP)", category: "Neural Networks" },
  { id: 9, question: "What function is applied to a node's output to introduce non-linearity into a neural network?", answer: "Activation Function", category: "Neural Networks" },
  { id: 10, question: "Which class of generative models creates images by learning to reverse a process of adding noise to a sample?", answer: "Diffusion Model", category: "Generative Models" },
  { id: 11, question: "What term describes unpredictable capabilities like reasoning that only appear when LLMs reach a certain scale?", answer: "Emergent Abilities", category: "LLM Concepts" },
  { id: 12, question: "What is the process of breaking down text into smaller units like words or subwords for numerical processing?", answer: "Tokenization", category: "NLP" },
  { id: 13, question: "Which subword tokenization algorithm iteratively merges the most frequent pairs of characters or bytes?", answer: "Byte Pair Encoding (BPE)", category: "NLP" },
  { id: 14, question: "What is a high-dimensional numeric vector that captures the semantic meaning of text or images?", answer: "Embedding", category: "Representations" },
  { id: 15, question: "The practice of crafting effective instructions to guide AI model output is known as _____.", answer: "Prompt Engineering", category: "Prompting" },
  { id: 16, question: "Which type of prompt defines the context, role, and boundaries for an LLM's behavior throughout an interaction?", answer: "System Prompt", category: "Prompting" },
  { id: 17, question: "Providing an LLM with instructions for a task without giving any specific examples is called _____.", answer: "Zero-shot Prompting", category: "Prompting" },
  { id: 18, question: "What technique involves providing a small number of input-output examples within a prompt to demonstrate a task?", answer: "Few-shot Prompting", category: "Prompting" },
  { id: 19, question: "Which prompting strategy involves assigning the AI a specific identity, such as 'Senior Python Developer'?", answer: "Role-Based Prompting", category: "Prompting" },
  { id: 20, question: "What technique improves reasoning by instructing the model to generate intermediate reasoning steps?", answer: "Chain-of-Thought (CoT)", category: "Reasoning" },
  { id: 21, question: "Which framework generalizes reasoning by allowing models to explore multiple reasoning paths simultaneously in a branching structure?", answer: "Tree of Thoughts (ToT)", category: "Reasoning" },
  { id: 22, question: "What prompting paradigm interleaves reasoning traces (thoughts) with task-specific actions like API calls?", answer: "ReAct (Reason + Act)", category: "Agents" },
  { id: 23, question: "Which decoding strategy samples multiple reasoning paths and selects the most frequent answer via majority vote?", answer: "Self-Consistency", category: "Inference" },
  { id: 24, question: "What method prompts an LLM to answer a high-level general question before addressing a specific user query?", answer: "Step-Back Prompting", category: "Prompting" },
  { id: 25, question: "What technique involves the model generating relevant facts about a topic to use as context for a final answer?", answer: "Generated Knowledge Prompting", category: "Prompting" },
  { id: 26, question: "What training method uses the input data itself (e.g., predicting the next word) to create supervisory signals?", answer: "Self-Supervised Learning", category: "Training" },
  { id: 27, question: "What process calculates the gradients of the loss function to update model weights during training?", answer: "Backpropagation", category: "Training" },
  { id: 28, question: "Which optimization algorithm iteratively adjusts model weights to minimize the loss function?", answer: "Gradient Descent", category: "Training" },
  { id: 29, question: "What mathematical function quantifies the difference between a model's prediction and the actual target?", answer: "Loss Function", category: "Training" },
  { id: 30, question: "What is the process of further training a pre-trained foundation model on a targeted dataset for a specific task?", answer: "Fine-tuning", category: "Training" },
  { id: 31, question: "Which post-training method uses labeled instruction-response pairs to teach a model to follow specific styles?", answer: "Supervised Fine-Tuning (SFT)", category: "Fine-tuning" },
  { id: 32, question: "Methods that adapt large models by updating only a small number of extra parameters are categorized as _____.", answer: "PEFT (Parameter-Efficient Fine-Tuning)", category: "Fine-tuning" },
  { id: 33, question: "Which PEFT technique injects trainable low-rank matrices into the model while freezing the original weights?", answer: "LoRA (Low-Rank Adaptation)", category: "Fine-tuning" },
  { id: 34, question: "What training process aligns language models with human intent using a reward model based on human preferences?", answer: "RLHF (Reinforcement Learning from Human Feedback)", category: "Alignment" },
  { id: 35, question: "Which RL technique optimizes policy by comparing a group of outputs without using a separate critic model?", answer: "GRPO (Group Relative Policy Optimization)", category: "Alignment" },
  { id: 36, question: "What compression technique lowers memory usage by reducing the precision of model parameters (e.g., 32-bit to 4-bit)?", answer: "Quantization", category: "Optimization" },
  { id: 37, question: "What process involves training a smaller 'student' model to reproduce the behavior of a larger 'teacher' model?", answer: "Distillation", category: "Optimization" },
  { id: 38, question: "What phenomenon occurs when a neural network loses previously learned information while learning new data?", answer: "Catastrophic Forgetting", category: "Training" },
  { id: 39, question: "Which inference parameter controls the randomness of predictions by adjusting the probability distribution?", answer: "Temperature", category: "Inference" },
  { id: 40, question: "What sampling strategy selects the smallest set of tokens whose cumulative probability exceeds a threshold $P$?", answer: "Top-P (Nucleus Sampling)", category: "Inference" },
  { id: 41, question: "Which decoding method always selects the token with the highest probability at each step?", answer: "Greedy Decoding", category: "Inference" },
  { id: 42, question: "What is the term for an AI generating grammatically plausible but factually incorrect or ungrounded information?", answer: "Hallucination", category: "LLM Concepts" },
  { id: 43, question: "Which metric evaluates a model by checking if its output is perfectly identical to the reference answer?", answer: "Exact Match (EM)", category: "Evaluation" },
  { id: 44, question: "What metric quantifies text quality by comparing n-gram overlap between generated and reference translations?", answer: "BLEU Score", category: "Evaluation" },
  { id: 45, question: "Which set of metrics is primarily used to evaluate the quality of AI-generated summarization?", answer: "ROUGE", category: "Evaluation" },
  { id: 46, question: "What metric measures the probability that at least one of the top $k$ generated code samples passes unit tests?", answer: "Pass@k", category: "Evaluation" },
  { id: 47, question: "What evaluation pattern uses a stronger LLM to grade the performance and output of another model?", answer: "LLM-as-a-Judge", category: "Evaluation" },
  { id: 48, question: "What is the structured adversarial process of trying to provoke an AI into producing harmful or biased outputs?", answer: "Red Teaming", category: "Safety" },
  { id: 49, question: "What is the process of ensuring AI models are helpful, harmless, and honest according to human values?", answer: "AI Alignment", category: "Safety" },
  { id: 50, question: "Which framework retrieves information from an external vector database to ground an LLM's response?", answer: "Retrieval Augmented Generation (RAG)", category: "RAG" },
];

export const coursesData = [
  {
    id: 1,
    title: "Building Neural Networks",
    description: "A mechanic's guide to PyTorch. From tensors to training loopsâ€”mastering the mechanics of deep learning.",
    moduleCount: 12,
    progress: 45,
    status: "in-progress" as const,
    tag: "ENGINEERING TRACK",
  },
  {
    id: 2,
    title: "RAG Architectures",
    description: "Retrieval Augmented Generation on AWS. Architectures, options, and decision frameworks for enterprise data.",
    moduleCount: 8,
    progress: 0,
    status: "not-started" as const,
    tag: "ARCHITECTURE",
  },
  {
    id: 3,
    title: "Foundations of GenAI",
    description: "From deep learning architectures to enterprise scale. Understanding the architectural definitions and business value.",
    moduleCount: 15,
    progress: 100,
    status: "completed" as const,
    tag: "FOUNDATIONS",
  },
  {
    id: 4,
    title: "Building a Brain",
    description: "From the atomic perceptron to deep neural architectures. Solving the XOR problem and understanding backpropagation.",
    moduleCount: 6,
    progress: 0,
    status: "not-started" as const,
    tag: "NEURAL NETWORKS",
  },
  {
    id: 5,
    title: "The Goal: Loss Functions",
    description: "You cannot improve what you cannot measure. Deep dive into CrossEntropyLoss, MSELoss, and the loss landscape.",
    moduleCount: 4,
    progress: 0,
    status: "not-started" as const,
    tag: "OPTIMIZATION",
  },
  {
    id: 6,
    title: "Transformer Architecture",
    description: "Attention is all you need. Deconstructing the architecture that changed the world of NLP.",
    moduleCount: 10,
    progress: 0,
    status: "locked" as const,
    tag: "COMING SOON",
  },
];
