export interface Flashcard {
  id: number;
  question: string;
  answer: string;
  category?: string;
}

export const flashcardsData: Flashcard[] = [
  { id: 1, question: "What specialized field of AI focuses on creating entirely new content rather than just analyzing existing data?", answer: "Generative AI (GenAI)", category: "Foundations" },
  { id: 2, question: "What is a large-scale neural network trained on vast amounts of unlabeled data that can be adapted to many downstream tasks?", answer: "Foundation Model", category: "Foundations" },
  { id: 3, question: "A Large Language Model (LLM) is typically trained on datasets measured in what scale of size?", answer: "Terabytes", category: "Foundations" },
  { id: 4, question: "What type of AI model can simultaneously process and generate multiple data types like text, images, and video?", answer: "Multimodal Model", category: "Foundations" },
  { id: 5, question: "Which deep learning architecture uses self-attention mechanisms to handle long-range dependencies in sequential data?", answer: "Transformer", category: "Architecture" },
  { id: 6, question: "What component of the Transformer architecture allows the model to weigh the relative importance of different input parts?", answer: "Attention Mechanism", category: "Architecture" },
  { id: 7, question: "What is the fundamental building block of a neural network that acts as a simple binary classifier?", answer: "Perceptron", category: "Neural Networks" },
  { id: 8, question: "A feedforward neural network with fully connected neurons and hidden layers is known as a _____.", answer: "Multilayer Perceptron (MLP)", category: "Neural Networks" },
  { id: 9, question: "What function is applied to a node's output to introduce non-linearity into a neural network?", answer: "Activation Function", category: "Neural Networks" },
  { id: 10, question: "Which class of generative models creates images by learning to reverse a process of adding noise to a sample?", answer: "Diffusion Model", category: "Generative Models" },
  { id: 11, question: "What term describes unpredictable capabilities like reasoning that only appear when LLMs reach a certain scale?", answer: "Emergent Abilities", category: "LLM Concepts" },
  { id: 12, question: "What is the process of breaking down text into smaller units like words or subwords for numerical processing?", answer: "Tokenization", category: "NLP" },
  { id: 13, question: "Which subword tokenization algorithm iteratively merges the most frequent pairs of characters or bytes?", answer: "Byte Pair Encoding (BPE)", category: "NLP" },
  { id: 14, question: "What is a high-dimensional numeric vector that captures the semantic meaning of text or images?", answer: "Embedding", category: "Representations" },
  { id: 15, question: "The practice of crafting effective instructions to guide AI model output is known as _____.", answer: "Prompt Engineering", category: "Prompting" },
  { id: 16, question: "Which type of prompt defines the context, role, and boundaries for an LLM's behavior throughout an interaction?", answer: "System Prompt", category: "Prompting" },
  { id: 17, question: "Providing an LLM with instructions for a task without giving any specific examples is called _____.", answer: "Zero-shot Prompting", category: "Prompting" },
  { id: 18, question: "What technique involves providing a small number of input-output examples within a prompt to demonstrate a task?", answer: "Few-shot Prompting", category: "Prompting" },
  { id: 19, question: "Which prompting strategy involves assigning the AI a specific identity, such as 'Senior Python Developer'?", answer: "Role-Based Prompting", category: "Prompting" },
  { id: 20, question: "What technique improves reasoning by instructing the model to generate intermediate reasoning steps?", answer: "Chain-of-Thought (CoT)", category: "Reasoning" },
  { id: 21, question: "Which framework generalizes reasoning by allowing models to explore multiple reasoning paths simultaneously in a branching structure?", answer: "Tree of Thoughts (ToT)", category: "Reasoning" },
  { id: 22, question: "What prompting paradigm interleaves reasoning traces (thoughts) with task-specific actions like API calls?", answer: "ReAct (Reason + Act)", category: "Agents" },
  { id: 23, question: "Which decoding strategy samples multiple reasoning paths and selects the most frequent answer via majority vote?", answer: "Self-Consistency", category: "Inference" },
  { id: 24, question: "What method prompts an LLM to answer a high-level general question before addressing a specific user query?", answer: "Step-Back Prompting", category: "Prompting" },
  { id: 25, question: "What technique involves the model generating relevant facts about a topic to use as context for a final answer?", answer: "Generated Knowledge Prompting", category: "Prompting" },
  { id: 26, question: "What training method uses the input data itself (e.g., predicting the next word) to create supervisory signals?", answer: "Self-Supervised Learning", category: "Training" },
  { id: 27, question: "What process calculates the gradients of the loss function to update model weights during training?", answer: "Backpropagation", category: "Training" },
  { id: 28, question: "Which optimization algorithm iteratively adjusts model weights to minimize the loss function?", answer: "Gradient Descent", category: "Training" },
  { id: 29, question: "What mathematical function quantifies the difference between a model's prediction and the actual target?", answer: "Loss Function", category: "Training" },
  { id: 30, question: "What is the process of further training a pre-trained foundation model on a targeted dataset for a specific task?", answer: "Fine-tuning", category: "Training" },
  { id: 31, question: "Which post-training method uses labeled instruction-response pairs to teach a model to follow specific styles?", answer: "Supervised Fine-Tuning (SFT)", category: "Fine-tuning" },
  { id: 32, question: "Methods that adapt large models by updating only a small number of extra parameters are categorized as _____.", answer: "PEFT (Parameter-Efficient Fine-Tuning)", category: "Fine-tuning" },
  { id: 33, question: "Which PEFT technique injects trainable low-rank matrices into the model while freezing the original weights?", answer: "LoRA (Low-Rank Adaptation)", category: "Fine-tuning" },
  { id: 34, question: "What training process aligns language models with human intent using a reward model based on human preferences?", answer: "RLHF (Reinforcement Learning from Human Feedback)", category: "Alignment" },
  { id: 35, question: "Which RL technique optimizes policy by comparing a group of outputs without using a separate critic model?", answer: "GRPO (Group Relative Policy Optimization)", category: "Alignment" },
  { id: 36, question: "What compression technique lowers memory usage by reducing the precision of model parameters (e.g., 32-bit to 4-bit)?", answer: "Quantization", category: "Optimization" },
  { id: 37, question: "What process involves training a smaller 'student' model to reproduce the behavior of a larger 'teacher' model?", answer: "Distillation", category: "Optimization" },
  { id: 38, question: "What phenomenon occurs when a neural network loses previously learned information while learning new data?", answer: "Catastrophic Forgetting", category: "Training" },
  { id: 39, question: "Which inference parameter controls the randomness of predictions by adjusting the probability distribution?", answer: "Temperature", category: "Inference" },
  { id: 40, question: "What sampling strategy selects the smallest set of tokens whose cumulative probability exceeds a threshold P?", answer: "Top-P (Nucleus Sampling)", category: "Inference" },
  { id: 41, question: "Which decoding method always selects the token with the highest probability at each step?", answer: "Greedy Decoding", category: "Inference" },
  { id: 42, question: "What is the term for an AI generating grammatically plausible but factually incorrect or ungrounded information?", answer: "Hallucination", category: "LLM Concepts" },
  { id: 43, question: "Which metric evaluates a model by checking if its output is perfectly identical to the reference answer?", answer: "Exact Match (EM)", category: "Evaluation" },
  { id: 44, question: "What metric quantifies text quality by comparing n-gram overlap between generated and reference translations?", answer: "BLEU Score", category: "Evaluation" },
  { id: 45, question: "Which set of metrics is primarily used to evaluate the quality of AI-generated summarization?", answer: "ROUGE", category: "Evaluation" },
  { id: 46, question: "What metric measures the probability that at least one of the top k generated code samples passes unit tests?", answer: "Pass@k", category: "Evaluation" },
  { id: 47, question: "What evaluation pattern uses a stronger LLM to grade the performance and output of another model?", answer: "LLM-as-a-Judge", category: "Evaluation" },
  { id: 48, question: "What is the structured adversarial process of trying to provoke an AI into producing harmful or biased outputs?", answer: "Red Teaming", category: "Safety" },
  { id: 49, question: "What is the process of ensuring AI models are helpful, harmless, and honest according to human values?", answer: "AI Alignment", category: "Safety" },
  { id: 50, question: "Which framework retrieves information from an external vector database to ground an LLM's response?", answer: "Retrieval Augmented Generation (RAG)", category: "RAG" },
  { id: 51, question: "What specialized database is used to perform semantic similarity searches using high-dimensional embeddings?", answer: "Vector Database", category: "RAG" },
  { id: 52, question: "What is an LLM-powered system capable of planning, using tools, and executing multi-step tasks autonomously?", answer: "AI Agent", category: "Agents" },
  { id: 53, question: "What agent capability involves breaking a complex request down into a sequence of executable sub-tasks?", answer: "Planning", category: "Agents" },
  { id: 54, question: "In AI agents, what system component uses vector stores to retain information over long periods?", answer: "Long-term Memory", category: "Agents" },
  { id: 55, question: "What capability allows LLMs to generate structured JSON to invoke external APIs or functions?", answer: "Function Calling (Tool Use)", category: "Agents" },
  { id: 56, question: "What is the structured description provided to an LLM that explains how and when to use a specific external function?", answer: "Tool Definition", category: "Agents" },
  { id: 57, question: "What iterative design process involves optimizing system prompts and memory management for agent reliability?", answer: "Context Engineering", category: "Agents" },
  { id: 58, question: "What mechanism stores and reuses processed tokens to efficiently handle repetitive queries in large prompts?", answer: "Context Caching", category: "Optimization" },
  { id: 59, question: "In Python, which keywords are used to define a function and exit it with a specific value?", answer: "def / return", category: "Python" },
  { id: 60, question: "What Python keyword is used to create small, anonymous functions?", answer: "lambda", category: "Python" },
  { id: 61, question: "Which Python keyword allows a function to return a generator iterator that produces values over time?", answer: "yield", category: "Python" },
  { id: 62, question: "What keywords are used in Python for robust error handling and forcing specific exceptions?", answer: "try / except / raise", category: "Python" },
  { id: 63, question: "What debugging statement in Python tests a condition and raises an error if the condition is false?", answer: "assert", category: "Python" },
  { id: 64, question: "Which Python statement is used as a context manager, frequently for file handling?", answer: "with", category: "Python" },
  { id: 65, question: "In Python, what is the difference between a list and a dict?", answer: "A list is an ordered sequence, while a dict is a collection of key-value pairs.", category: "Python" },
  { id: 66, question: "Which Python function adds a counter to an iterable and returns it as an index-value pair?", answer: "enumerate", category: "Python" },
  { id: 67, question: "What Python function aggregates elements from multiple iterables into tuples?", answer: "zip", category: "Python" },
  { id: 68, question: "What is the fundamental multi-dimensional array structure in PyTorch optimized for GPU acceleration?", answer: "torch.tensor", category: "PyTorch" },
  { id: 69, question: "What PyTorch base class is used to define the layers and forward behavior of all neural network modules?", answer: "nn.Module", category: "PyTorch" },
  { id: 70, question: "In PyTorch, which method is used to specify how input passes through the network layers?", answer: "forward()", category: "PyTorch" },
  { id: 71, question: "In PyTorch, which method is used to compute the gradients required for backpropagation?", answer: "backward()", category: "PyTorch" },
  { id: 72, question: "What are the raw, unnormalized scores output by the last layer of a neural network called?", answer: "logits", category: "Neural Networks" },
  { id: 73, question: "Which PyTorch utility is an abstract class representing a dataset?", answer: "Dataset", category: "PyTorch" },
  { id: 74, question: "Which PyTorch utility handles the loading, batching, and shuffling of data?", answer: "DataLoader", category: "PyTorch" },
  { id: 75, question: "Which Hugging Face class automatically selects the correct tokenizer for a given pre-trained model?", answer: "AutoTokenizer", category: "Hugging Face" },
  { id: 76, question: "Which Hugging Face class is used to instantiate a model specifically for text generation?", answer: "AutoModelForCausalLM", category: "Hugging Face" },
  { id: 77, question: "What method do Hugging Face models use to produce text sequences by predicting tokens iteratively?", answer: "generate()", category: "Hugging Face" },
  { id: 78, question: "Which specialized Hugging Face trainer class is used specifically for Supervised Fine-Tuning?", answer: "SFTTrainer", category: "Hugging Face" },
  { id: 79, question: "Which specialized Hugging Face trainer class is used for Group Relative Policy Optimization?", answer: "GRPOTrainer", category: "Hugging Face" },
  { id: 80, question: "What is the base class for creating individual units of testing in Python's built-in testing framework?", answer: "TestCase", category: "Python" },
];

import pytorchThumbnail from "@/assets/pytorch-thumbnail.png";
import ragThumbnail from "@/assets/rag-thumbnail.png";
import safetyThumbnail from "@/assets/safety-thumbnail.png";
import machineryThumbnail from "@/assets/machinery-thumbnail.png";
import foundationThumbnail from "@/assets/foundation-thumbnail.png";
import genaiThumbnail from "@/assets/genai-thumbnail.png";

export const coursesData = [
  { id: 1, title: "Building Neural Networks", description: "A mechanic's guide to PyTorch. From tensors to training loops.", moduleCount: 15, progress: 0, status: "not-started" as const, tag: "PYTORCH", imageUrl: pytorchThumbnail },
  { id: 2, title: "RAG Architectures", description: "Retrieval Augmented Generation on AWS. Decision frameworks for enterprise.", moduleCount: 15, progress: 0, status: "not-started" as const, tag: "ARCHITECTURE", imageUrl: ragThumbnail },
  { id: 3, title: "Foundations of GenAI", description: "From deep learning to enterprise scale. Architectural definitions and business value.", moduleCount: 15, progress: 0, status: "not-started" as const, tag: "FOUNDATIONS", imageUrl: genaiThumbnail },
  { id: 4, title: "The Machinery of Thought", description: "From probabilistic autocomplete to System 2 reasoning. LLMs and Diffusion Models.", moduleCount: 15, progress: 0, status: "not-started" as const, tag: "NEURAL NETWORKS", imageUrl: machineryThumbnail },
  { id: 5, title: "Engineering Responsibility", description: "The mechanics, ethics, and safety of Generative AI.", moduleCount: 15, progress: 0, status: "not-started" as const, tag: "SAFETY", imageUrl: safetyThumbnail },
  { id: 6, title: "GenAI Practice & Governance", description: "Workflows, application, and oversight. From NIST to UNESCO.", moduleCount: 14, progress: 0, status: "not-started" as const, tag: "GOVERNANCE", imageUrl: foundationThumbnail },
  { id: 7, title: "Foundation Model Mastery", description: "From zero-shot to advanced reasoning agents. Prompt engineering guide.", moduleCount: 15, progress: 0, status: "not-started" as const, tag: "PROMPTING", imageUrl: foundationThumbnail },
  { id: 8, title: "Foundation Models: Strategic", description: "The paradigm shift, open revolution, and multimodal frontier.", moduleCount: 15, progress: 0, status: "not-started" as const, tag: "STRATEGY", imageUrl: foundationThumbnail },
  { id: 9, title: "Anatomy of Deep Learning", description: "Building a brain from scratch. Perceptrons to backpropagation.", moduleCount: 13, progress: 0, status: "not-started" as const, tag: "DEEP LEARNING", imageUrl: machineryThumbnail },
  { id: 10, title: "GenAI Evaluation Methods", description: "From exact match to LLM-as-a-judge. A practitioner's toolkit.", moduleCount: 13, progress: 0, status: "not-started" as const, tag: "EVALUATION", imageUrl: safetyThumbnail },
  { id: 11, title: "Scripting AI Personas", description: "Role-based prompting. Crafting specialized personas for quality outputs.", moduleCount: 15, progress: 0, status: "not-started" as const, tag: "PROMPTING", imageUrl: genaiThumbnail },
  { id: 12, title: "LLMs to AI Agents", description: "From adaptation to autonomous action. RAG, fine-tuning, and agents.", moduleCount: 15, progress: 0, status: "not-started" as const, tag: "AGENTS", imageUrl: pytorchThumbnail },
];
